---
title: 「学习笔记」AI大模型基础
date: 2025-12-22 18:32:50
tags: [AI大模型, LLM, 学习笔记]
categories: [AI大模型]
series: AI大模型
toc: true
math: true
---


## 1. AI基础概念
- **人工智能（Artificial Intelligence, AI）**：最宏观的概念，一个庞大的科学领域，致力于创造能够执行通常需要人类智能的任务的机器系统。（AI不一定需要学习，早期AI大多基于预设的规则和逻辑。）
- **机器学习（Machine Learning, ML）**：实现AI的一种**核心方法**。其理念是：**从数据中“学习”规律，而不是硬编码规则**。其性能随着经验（数据）的增加而提高。
- **深度学习（Deep Learning, DL）**：机器学习的一个**重要分支**。它使用更深、更复杂的**神经网络**结构来学习数据。DL的强大之处在于能自动从原始数据中提取特征。
- **神经网络（Neural Network, NN）**：受人类大脑启发而构建的**算法模型**，是深度学习的**基础和核心架构**。它由大量相互连接的“神经元”节点组成。接收输入，进行加权求和，并通过激活函数产生输出。
- **大模型（Large Models, LM）**：通常指基于**深度学习**和**大规模神经网络**（如Transformer），参数规模巨大（通常达到**十亿**或**万亿**级别）的深度学习模型。大模型是当前深度学习技术发展到极致的体现。
- **大语言模型（Large Language Model, LLM）**：LM最突出的一个**子类**，特指那些专门在**海量文本数据**上训练，专门用于**理解和生成自然语言**的大模型。

它们之间的包含关系：`[人工智能]-->[机器学习]-->[深度学习]-->[神经网络]-->[大模型]`。
- `[大模型]`包含：
    - `[大语言模型]`：处理文本
    - `[多模态大模型]`：处理文本、图像、音频等
    - `[科学大模型]`：处理科学数据


### 1.1 AI发展关键里程碑
| 发展阶段 | 时间跨度 | 核心范式 | 关键技术/特征 | 标志性里程碑事件 |
| :--- | :--- | :--- | :--- | :--- |
| 第一阶段<br>Logic (逻辑) | 1950s<br>- 1970s | 符号主义<br>(Symbolism) | 规则驱动：<br>通过硬编码的逻辑规则（如果-那么）让机器模拟推理。 | 1950年：图灵提出“图灵测试”，定义机器智能。<br>1956年：达特茅斯会议确立“人工智能”术语。<br>1966年：ELIZA诞生，首个模拟心理医生的聊天机器人。 |
| 过渡阶段<br>Knowledge (知识) | 1980s<br>- 1990s | 专家系统<br>(Expert Systems) | 知识库：<br>将人类专家的经验转化为计算机可查询的知识库。 | 1980年代：XCON等专家系统在商业领域（如医疗诊断、设备配置）取得成功。 |
| 第二阶段<br>Learning (学习) | 2006年<br>- 2020年 | 深度学习<br>(Deep Learning) | 数据驱动：<br>利用神经网络从海量数据中自动提取特征，进行分类和识别。 | 2012年：AlexNet在图像识别竞赛中大胜，引爆深度学习革命。<br>2016年：AlphaGo击败李世石，展示复杂决策能力。<br>2017年：Google提出Transformer架构（大模型基石）。 |
| 第三阶段<br>Generating (生成) | 2020年<br>- 至今 | 大模型/生成式AI<br>(Generative AI) | 生成创造：<br>基于超大规模预训练，具备通用任务处理能力，能创造文本、图像、视频。 | 2020年：GPT-3展现惊人的零样本学习能力。<br>2022年：ChatGPT发布，引爆全球AI应用潮。<br>2024年：Sora、GPT-4o实现高质量视频生成与实时多模态交互。 |

1. 从“教”到“学”再到“创”：
    + Logic时代，我们像严厉的老师，手把手教机器每一步规则（编程）。
    + Learning时代，我们变成了教练，给机器看大量教材（数据），让它自己总结解题方法（训练模型）。
    + Generating时代，机器变成了通才助手，它读完了全网知识，不仅能答题，还能帮你写报告、画插画、写代码（生成内容）。
2. 在“Logic”和“Learning”之间的过渡阶段，那时候是“Knowledge is Power”（知识就是力量），人们试图把人类专家的知识库直接灌输给机器；但因为知识库太难维护，最终衰落，直到2010年左右大数据和算力突破，才迎来了真正的“Learning”时代。
3. **Transformer是分水岭**：虽然深度学习在2006年就复兴了，但直到 2017年 Transformer 架构出现，才真正为后来的“大模型”和“生成式AI”铺平了道路。它是连接“Learning”和“Generating”的关键桥梁。
4. **2022年是奇点**：2022年 ChatGPT 的发布是第三次浪潮的爆发点。它让AI从实验室和服务器里走了出来，变成了每个人都能用的生产力工具。



## 2. 神经网络（Neural Networks）
神经网络（Neural Networks, NN）本质上是一种受生物神经元启发的**多层非线性数学模型**。它通过堆叠可学习的权重层和激活函数，实现对复杂数据的表示与预测。

### 2.1 神经元 (Neuron)
神经网络是由大量相互连接的“人工神经元”组成的计算系统。
- **神经元 (Neuron)**：计算加权输入+偏置(z=w⋅x+b)，通过激活函数输出。
- **权重 (Weights)**：连接神经元，决定特征重要性，是模型学习的参数。
- **激活函数**：引入非线性，使网络能拟合任意复杂函数。
  + **ReLU**(Rectified Linear Unit)： 目前最常用。公式是`f(x)=max(0,x)`。**隐层首选**，计算快，能有效缓解梯度消失问题，就像一个“大于0就放行”的过滤器。
  + Sigmoid：输出在`(0,1)`之间，表示概率，常用于二分类问题的输出层。
  + Softmax：输出概率归一化，和为1（所有类别概率加起来等于1），常用于多分类任务的输出层，能把输出转换为概率分布。

### 2.2 整体架构
一个典型的神经网络通常由以下**三层**构成：

| 层级 | 作用 | 比喻 |
| :--- | :--- | :--- |
| 输入层 | 接收原始数据（如图片像素、文字向量）。 | 原材料仓库 |
| 隐藏层 | 进行复杂的数学运算，提取特征（如从像素中识别出边缘，再识别出形状）。 | 加工车间（可以有很多层） |
| 输出层 | 给出最终结果（如“这是猫”、“情感倾向：正面”）。 | 成品包装区 |


### 2.3 核心工作流：前向传播与反向传播
神经网络的训练过程是一个 “**预测-纠错**” 的循环：
- 🚀 前向传播(Forward Propagation)：数据从输入层流向输出层，逐层计算传递。
    + \( \text{输入层} → h_1 = σ(W_1x + b_1) \)
    + \( \text{隐藏层} → h_l = σ(W_l h_{l-1} + b_l) \)
    + \( \text{输出层} → hat{y} = f(W_L h_{L-1} + b_L) \)
    + **注：** 其中 σ 通常为 ReLU 等非线性激活函数， f 为输出层激活函数（如 Softmax）。
- 🔙 反向传播(Back Propagation)：利用链式法则，从输出层向输入层回溯，计算损失对每个权重的梯度：\( \frac{\partial L}{\partial W} = \frac{\partial L}{\partial h} \cdot \frac{\partial h}{\partial W} \)
  - **关键步骤：**
    1.  **计算误差：** 计算输出层误差 \( \delta_L \)。
    2.  **误差回传：** 反向传播误差至前层：\( \delta_l = (W_{l+1}^T \delta_{l+1}) \odot \sigma'(z_l) \)。
    3.  **参数更新：** 计算梯度并更新权重。


### 2.4 网络架构演进
- **全连接网络 (FCN/MLP)**： 最基础的架构，每层神经元与下一层全连接。适合小规模结构化数据（如表格数据）。
- **卷积神经网络 (CNN)**： 利用局部连接和权重共享，擅长捕捉图像、视频中的空间特征（如边缘、纹理）。
- **循环神经网络 (RNN/LSTM/GRU)**： 具有记忆能力，擅长处理序列依赖问题（如文本、语音、时间序列）。
- **Transformer**： 基于自注意力机制，摒弃了循环结构，擅长并行处理长序列数据，是当前大语言模型（LLM）的基石。


### 2.5 神经网络总结
神经网络的核心在于**层次化的特征提取**（浅层抓细节，深层抓语义）和**端到端的误差反向传播**。
- 核心开发流通常是：
  + **数据预处理 → 搭建模型骨架 (CNN/RNN/Transformer) → 定义损失与优化器 → 前向传播计算 Loss → 反向传播更新参数 → 验证与调优。**



## 3. 词元（Token）
`Token`(词元)是大模型处理数据的**最小单位**。AI不读字，读的是Token(整数编号)。计算机无法直接理解文字，必须将文字转换为数字向量。Tokenization（分词）过程就是将原始文本切片，然后映射到一个巨大的数字索引表中。
- 英文单词可能被拆分，常见词根ing、ed会被切分为独立Token
  + 例如：英文 "unhappiness" 可能被切分为 ["un", "happy", "ness"]。
- 中文通常一个汉字对应一个Token，相比英文，中文的Token密度会更高，表达效率也不同。
  + 例如：中文 “人工智能” 可能被切分为 ["人工", "智能"] 或 ["人", "工", "智", "能"]。

### 3.1 上下文窗口（Context Window）
- **Token定义：** Token 是自然语言在**分词器**作用下，映射到高维向量空间的离散化索引。它是模型输入输出的最小语义单元，也是计算成本的原子单位。
    - **分词算法：** 现代大模型多采用 **Byte Pair Encoding** 或 **SentencePiece**。它们通过统计词频，将高频的字节对或字符对不断合并，形成一个混合了单词、子词和单字的词表。
    - **信息密度：** 一个 Token 承载的信息量是不均等的。例如，“的”承载的语义信息极少，而一个专业术语（如“Transformer”）承载的信息量极大。
- **上下文窗口**（Context Window）是模型在一次前向传播中，能够访问的最大`Token`序列长度。它定义了模型的**工作记忆**容量。
    - **技术瓶颈：** 核心在于自注意力机制的计算复杂度为 O(N^2)，显存占用（特别是 Key-Value Cache）也随序列长度线性或平方级增长。
    - **位置外推：** 模型在训练时通常有一个固定的上下文长度（如 8k），通过调整位置编码（如 NTK-by-parts, Linear Scaling），可以让模型在推理时支持更长的上下文（如 32k 或 128k），但这通常以牺牲长距离依赖的精度为代价。
    - **KV Cache：** 为了加速自回归生成，模型会缓存每一层的 Key 和 Value 矩阵。随着对话轮次增加，KV Cache 会占用巨大的显存，这是限制长对话的主要物理因素。
    - **“中间遗忘”效应：** 研究表明，模型对输入序列的**开头**和**结尾**部分记忆最深刻，而对**中间**部分的记忆相对薄弱。这被称为“序列位置偏差”。

### 3.2 Token扩展
- 大模型的本质：规模化的“下一个Token预测”。
- **Token 与 Embedding：** Token 本身只是一个 ID，必须通过 **Embedding Lookup Table** 转换为稠密的向量（Embedding），模型才能进行数学运算。这个向量空间的维度通常在几千维。
- **多模态 Token：** 在多模态模型中，图像被分割为“图像块”，通过 Vision Transformer 编码为视觉 Token；音频被编码为声学 Token。最终所有模态都统一在同一个 Token 空间中进行处理。



## 4. 温度（Temperature）与 幻觉（Hallucination）
### 4.1 温度（Temperature） —— 概率分布的锐化系数
**温度（Temperature）** 是一个关键超参数，控制生成结果的**熵**。用于调节模型生成文本的随机性和创造性。其值通常在0到2之间，数值越低输出越确定和稳定，数值越高则越多样化和富有创意。
- **作用原理**：温度通过调整模型输出的概率分布来控制词选择的随机性。低温度（如0.1-0.3）放大高概率词的优势，抑制低概率词，使输出更“保守”；高温度（如0.8-2.0）平滑概率分布，增加低概率词被选中的机会，使输出更“放飞”。
- **典型场景**：
  - **低温（0-0.3）**：适合需要准确性和稳定性的任务，如事实问答、代码生成或数学解题，输出更“老实”但可能呆板。
  - **中温（0.4-0.7）**：平衡准确与自然，适用于日常聊天或文章摘要，输出更易读和亲切。
  - **高温（0.8-2.0）**：激发创意，适合故事创作、诗歌或头脑风暴，输出可能新颖但易出现逻辑错误或“胡说八道”。
- **设置建议**：根据任务需求调整温度，避免“温度越高越聪明”的误区。创意任务可搭配Top-p等参数协同优化，但需注意高温可能增加幻觉风险。
- **Top-k 与 Top-p：** 除了温度，还有更高级的采样策略。
  - **Top-k：** 只从概率最高的 k 个词中采样。
  - **Top-p：** 从累积概率超过 p 的最小词集中采样（也叫核采样）。这些方法通常比单纯调整温度更能保证生成文本的质量。

### 4.2 幻觉（Hallucination） —— 概率与事实的错位
**“幻觉”（Hallucination）** 指的是大模型“一本正经地胡说八道”。指模型生成的内容在事实性、逻辑性或忠实度上与真实世界或输入上下文不一致的现象。
- **产生幻觉的原因** ：
    + **概率游戏而非事实库**：大模型的核心是预测“下一个最可能出现的词”。
    + **训练数据的缺陷**：模型的知识源于海量的互联网数据，这些数据本身就可能包含错误、过时信息或矛盾之处。模型缺乏辨别真伪的能力，可能会把虚构的内容当成事实学习。
    + **“过度自信”的对齐机制**：在训练的最后阶段（RLHF），为了讨好人类反馈，模型学会了表现得自信和乐于助人。这导致它倾向于给出一个看似完美的回答，即使这个回答是编造的。
    + **复杂推理的失误**：在进行长链条的逻辑推理或数学计算时，模型可能在某一步骤出错，导致后续结果完全偏离轨道。
    + **知识截止日期：** 模型的知识固化在训练数据中，无法感知训练日期之后发生的事件。
    + **越狱攻击：** 攻击者通过精心设计的提示词，利用长上下文窗口中的信息，诱导模型绕过对齐机制（Alignment），生成有害或虚假内容。
- **幻觉缓解策略：**
    + **RAG：** 检索增强生成，通过外部知识库实时检索信息注入上下文，弥补模型内部知识的不足。
    + **事实性校验：** 在生成过程中引入事实核查模块，或使用基于强化学习的对齐技术。

### 4.3 温度对幻觉的影响
大模型的温度参数（Temperature）直接影响生成文本的随机性和幻觉概率：温度越低，模型输出越保守、幻觉越少；温度越高，随机性越大，幻觉风险显著增加。
- **低温度（如 0.1-0.5）**：概率分布更集中，模型优先选择高置信度词汇，减少虚构内容。例如，在事实问答或文档总结等需高准确性的任务中，低温设置可将幻觉率降至最低。
- **高温度（如 >1.0）**：概率分布更平滑，低概率词汇被激活，导致输出更具创造性但易产生幻觉。例如，温度设为 1.5 时，模型可能生成逻辑合理但事实错误的内容。
- **极端温度（=0 或 >2）**：温度为 0 时触发贪婪采样（仅选最高概率词），但若训练数据有误，仍可能输出错误答案；温度过高则放大尾部 token 概率，加剧长尾错误。

最佳实践建议
- **场景化设置**：
  - 严谨任务（如医疗诊断、金融分析）：使用低温（0.2-0.5）以抑制幻觉。
  - 创意任务（如头脑风暴）：可适度提高温度（1.0-1.5），但需配合事实核查。
- **结合其他方法**：低温虽有效，但无法彻底消除幻觉（因训练数据缺陷仍存在）。建议与检索增强生成（RAG）、提示词优化等技术结合，进一步降低风险。
- 注：当前主流模型（如 DeepSeek）通常将 T=0.3~0.7 设为严谨模式，T=1.5~2.0 为创意模式。



## 总结
如果将大模型视为一个**计算机系统**：
1.  **输入层：** 输入的提示（`Prompt`）被 **Tokenizer** 切割并映射为 `Token ID` 流。
2.  **内存层：** 这些 `Token ID` 及其对应的 `Embedding` 向量被加载到 **上下文窗口**（有限的内存空间）中。
3.  **计算层：** **Transformer** 架构通过多层自注意力和前馈网络，对这些向量进行复杂的非线性变换，计算出下一个 `Token` 的概率分布。
4.  **控制层：** **Temperature** 参数调节这个概率分布的锐化程度，决定是“死板照抄”还是“自由发挥”。
5.  **输出层：** 模型根据调整后的概率采样出下一个 `Token`，并将其反馈回输入端，形成自回归循环。
6.  **风险层：** 在整个过程中，由于训练数据的局限性和概率生成的本质，**幻觉**始终是一个潜在的系统级 `Bug`。

