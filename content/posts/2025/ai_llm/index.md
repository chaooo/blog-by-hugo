---
title: 「学习笔记」AI基础 & 大语言模型（LLM）原理
date: 2025-11-28 16:42:47
tags: [AI大模型, LLM, 学习笔记]
categories: [AI大模型]
series: AI大模型
toc: true
---


## AI基础概念
- **人工智能（Artificial Intelligence, AI）**：最宏观的概念，一个庞大的科学领域，致力于创造能够执行通常需要人类智能的任务的机器系统。（AI不一定需要学习，早期AI大多基于预设的规则和逻辑。）
- **机器学习（Machine Learning, ML）**：实现AI的一种**核心方法**。其理念是：**从数据中“学习”规律，而不是硬编码规则**。其性能随着经验（数据）的增加而提高。
- **深度学习（Deep Learning, DL）**：机器学习的一个**重要分支**。它使用更深、更复杂的**神经网络**结构来学习数据。DL的强大之处在于能自动从原始数据中提取特征。
- **神经网络（Neural Network, NN）**：受人类大脑启发而构建的**算法模型**，是深度学习的**基础和核心架构**。它由大量相互连接的“神经元”节点组成。接收输入，进行加权求和，并通过激活函数产生输出。
- **大模型（Large Models, LM）**：通常指基于**深度学习**和**大规模神经网络**（如Transformer），参数规模巨大（通常达到**十亿**或**万亿**级别）的深度学习模型。大模型是当前深度学习技术发展到极致的体现。
- **大语言模型（Large Language Model, LLM）**：LM最突出的一个**子类**，特指那些专门在**海量文本数据**上训练，专门用于**理解和生成自然语言**的大模型。

它们之间的包含关系：`[人工智能]-->[机器学习]-->[深度学习]-->[神经网络]-->[大模型]`。
- `[大模型]`包含：
    - `[大语言模型]`：处理文本
    - `[多模态大模型]`：处理文本、图像、音频等
    - `[科学大模型]`：处理科学数据


## 大语言模型
**大语言模型**（Large Language Model, **LLM**）是基于**深度学习**技术构建的人工智能系统。它通过在海量文本数据上进行训练，学习语言的统计规律、语法结构、语义关系以及世界知识，从而具备处理和生成人类语言的能力。
- **核心原理**：以Transformer架构为计算引擎，通过海量文本数据上的自监督学习，基于“预测下一个词元（token）”的任务目标，最终构建出一个能处理和生成语言的复杂概率模型。
- **核心技术**：现代LLM普遍采用**Transformer架构**。其核心组件**自注意力机制**（Self-Attention Mechanism）能动态计算句子中每个词之间的关系权重，从而有效捕捉长距离依赖和上下文语义。
- **核心能力**：
    - **文本生成**：生成连贯文本、进行多轮对话、创造性写作
    - **语言理解**：执行文本分类、情感分析、信息抽取等任务
    - **逻辑推理**：解决逻辑题、常识推理、执行基于模式的数学运算等
    - **代码处理**：生成、解释、调试程序代码


## 1. 概率预测与文本生成
LLM本质上是一个高度复杂的“自动补全”系统，其基本原理如下：
- **核心任务**：给定一段文本（称为“上下文”或“提示”），模型计算下一个最可能出现的词（token）的概率分布。
- **生成过程**：文本生成是一个“预测—添加—再预测”的循环过程：模型预测下一个词，将其加入上下文，继续预测下一个词，直到生成完整内容。每次预测都基于复杂的概率计算。
- **概率建模**：模型学习的是条件概率分布 P(token | context)，即“在已有文本条件下，每个词出现的概率”。该分布通过深度神经网络建模。


## 2. Transformer架构详解
Transformer由Google于2017年在论文《Attention Is All You Need》中首次提出，彻底改变了自然语言处理领域。它完全基于**自注意力机制**建模序列，能高效捕捉长距离依赖关系。
### 2.1 架构背景与创新价值
在Transformer出现前，主流模型如RNN、LSTM等采用循环处理方式，存在明显局限：
- **顺序依赖**：必须按顺序逐步处理，无法并行计算
- **计算效率低**：长序列处理速度慢
- **长程依赖衰减**：远距离词语间关系难以捕捉

Transformer的突破性在于：
- **完全并行化**：可同时处理整个序列，大幅提升训练速度
- **全局依赖建模**：任意两个词之间可直接建立联系
- **可扩展性强**：架构设计支持构建超大规模模型


### 2.2 整体架构设计
Transformer采用**编码器-解码器**堆叠设计：
- **编码器堆叠**：由多个相同结构的编码器层组成，负责提取输入文本的上下文特征，输出语义表示。
- **解码器堆叠**：同样由多个解码器层组成，基于编码器输出和已生成的内容，逐步生成目标文本。
- 编码器-解码器**连接**：通过**交叉注意力机制**（Cross-Attention），解码器在生成每个词时关注编码器的输出。
- **宏观流程**：`输出序列 = 解码器（编码器（输入）+ 已生成序列）`


### 2.3 核心组件
#### 2.3.1 位置编码（Positional Encoding）
- **词嵌入（Word Embedding）**：将每个词转化为高维向量，以捕获语义信息。
- **位置编码（Positional Encoding）**：因为自注意力本身不包含位置信息，需额外加入顺序信息（例如用正弦函数或可学习编码）。
- **最终输入**：`输入向量 = 词嵌入 + 位置编码`

#### 2.3.2 自注意力机制（Self-Attention）
自注意力让句子中每个词都能关注所有其他词，并动态计算权重。
- **计算步骤（缩放点积注意力）**：
  1. **生成Q、K、V矩阵**：对输入向量做三种线性变换，得到查询(Query)、键(Key)、值(Value)矩阵
  2. **计算注意力分数**：`分数 = Q · K^T`，（衡量查询和键的相似度）
  3. **缩放分数**：`缩放分数 = 分数 / sqrt(d_k)`（防止数值过大，d_k为键向量维度）
  4. **Softmax归一化**：`注意力权重 = softmax(缩放分数)`，（转为权重概率分布）
  5. **加权求和**：`输出 = 注意力权重 · V`，（生成上下文感知的表示）
- **矩阵运算优势**：整个序列的Q、K、V可打包成矩阵一次性计算，效率极高。

#### 2.3.3 多头注意力（Multi-Head Attention）
通过多个注意力头并行工作，从不同子空间捕获不同类型的依赖关系：
- **实现机制**：将Q、K、V投影到多个子空间，每个头(子空间)独立计算注意力，拼接结果并融合
- **作用**：让模型同时关注语法、语义、指代等多种信息，增强表达能力。

#### 2.3.4 位置式前馈网络
对每个位置的表示进行独立非线性变换：
- **公式**：`FFN(x) = max(0, xW₁ + b₁)W₂ + b₂`（使用ReLU激活）
- **作用**：为每个位置提供额外的非线性变换能力，增强模型表达能力
- **特点**：对每个位置独立处理，参数共享

#### 2.3.5 残差连接与层归一化
- **残差连接**：在每层前后添加“输入→输出”的直连路径，**输出 = LayerNorm(x + Sublayer(x))**，缓解梯度消失
- **层归一化**：对每个样本的所有特征做归一化，稳定训练

#### 2.3.6 解码器的掩码自注意力
- **作用**：防止解码器在训练时“偷看”未来信息
- **实现**：通过掩码矩阵（mask）将未来位置的注意力权重设为0

#### 2.3.7 编码器-解码器交叉注意力
- **作用**：连接编码器和解码器，让解码器关注输入中的相关信息
- **来源**：Query来自解码器，Key和Value来自编码器输出
- **应用**：在翻译、摘要等任务中至关重要


## 3. 训练过程
LLM的训练分两大阶段：预训练（Pre-training）和微调（Fine-tuning），依赖海量文本数据与强大算力（如GPU/TPU集群）。

### 3.1 预训练 (Pre-training) - 构建基础模型
预训练是LLM训练的核心阶段，目标是通过自监督学习，让模型在大规模文本上学习语言规律、知识和基础推理能力。
- **核心任务**：**下一个词元预测** (Next Token Prediction)：模型根据上文预测下一个词，训练目标：让预测尽可能准确。
- **技术实现**：**自回归**(Autoregressive) 与 **自编码**(Autoencoding)
    - **自回归 (AR) 模型**（如GPT系列）：传统语言模型，从左到右逐个生成词，训练目标是最大化序列概率。
    - **自编码 (AE) 模型**（如BERT）：掩码语言模型，随机遮盖部分词，训练模型根据上下文预测被遮词（**更擅长语言理解任务，而非开放式文本生成**）
- **训练数据与规模**：TB级文本（来自书籍、网页、代码等），需清洗、去重、过滤有害内容。
- **损失函数**：交叉熵损失（Cross-Entropy Loss）衡量模型预测与真实标签的差异，**公式**：`Loss = -Σ y_i * log(p_i)`。

### 3.2 指令微调 (Instruction Fine-Tuning) - 对齐人类意图
预训练模型知识丰富，但不一定懂人类指令。指令微调的目标是让模型输出更符合人类期望。
- **目的**：让模型学会理解并执行指令（如问答、翻译、摘要等），从“知识库”变成“助手”。
- **数据形式**：**指令—输出对**（人工编写或半自动生成），例：指令“解释牛顿第一定律”，输出“牛顿第一定律，又称惯性定律……”
- **训练过程**：**监督微调 (Supervised Fine-Tuning, SFT)**。输入指令，训练模型生成对应输出。损失函数与预训练相同（**通常只计算模型输出部分的损失，即指令后的文本**）。

### 3.3 对齐微调 (Alignment Tuning) - 优化输出偏好
即使经过SFT（监督微调），模型输出仍可能不准确或有害。对齐微调通过人类反馈进一步优化模型。
- **主流方法**：RLHF（基于人类反馈的强化学习），这是OpenAI等机构采用的关键技术
  - **步骤**：
    1. 收集人类偏好数据：对同一问题，标注员对多个模型输出排序（如A > B）
    2. 训练奖励模型（Reward Model, RM）：学习预测人类偏好，输出奖励分数
    3. 强化学习优化策略模型：使用PPO等算法，训练模型输出高分结果（同时避免偏离原始模型太远）
- **替代方案**：DPO（直接偏好优化），直接利用偏好数据优化模型，无需训练奖励模型；更简单稳定，逐渐成为主流。

### 3.4 关键训练技术与挑战
- **分布式训练**：模型太大（可达万亿参数），需用数据并行、模型并行等技术在多GPU/TPU上协同训练
- **优化器**：常用AdamW或Lion等自适应优化器
- **学习率调度**：训练初期逐渐提高学习率（热身），后期逐步降低
- **计算成本**：训练顶尖模型需数百万美元和数周至数月时间
- **挑战**：灾难性遗忘、训练稳定性、输出安全、数据偏见等


## 4. 能力涌现 (Emergent Abilities)
模型达到一定规模（参数量、数据量、算力）后，突然表现出小模型不具备的新能力，这些能力并非人为设计，而是从大规模训练中“自然浮现”。
### 4.1 表现形式
1. **上下文学习（In-Context Learning, ICL）**：通过几个示例（如“英文→中文”翻译案例），模型无需训练就能执行新任务。
2. **指令遵循**：模型能理解并响应自然语言指令（如“写一封病假邮件”）。
3. **逐步推理（Chain-of-Thought Reasoning）**：模型展示多步推理过程，解决逻辑或数学问题（如数学应用题）。
4. **代码能力**：理解、生成、解释代码的能力在代码训练后自然出现。

### 4.2 表现形式
- **规模效应**：模型性能与规模（参数、数据、算力）呈幂律关系，达到临界点后新能力涌现。
- **量变到质变**：大规模参数和训练数据让模型内部形成复杂表示，**能够学习并泛化出更抽象和复杂的任务模式**。

### 4.3 局限性
尽管LLM能力强大，但仍存在明显局限：
- **幻觉**：生成看似合理但不真实的内容，因为模型目标是“生成流畅文本”，而非“保证正确”。
- **缺乏真正理解**：本质是统计模式匹配，无法像人类一样理解语义或物理世界。
- **推理能力不稳定**：逻辑和数学能力时好时坏，**对复杂、新颖问题的推理能力有限**。
- **上下文长度限制**：模型处理长文本能力有限，影响生成一致性和信息完整性。
- **知识滞后**：训练数据截止后新信息无法获取，需借助外部检索（如RAG）更新。
- **偏见与毒性**：可能从训练数据中学习并放大社会偏见和有害观点，可能生成刻板印象、歧视性内容，或对特定群体做出不公平的决策。


## 总结
大语言模型（LLM）本质是一台强大的统计机器，而非有意识的“大脑”。以Transformer为引擎，通过海量文本进行“下一个词预测”的预训练，学习语言和知识模型，再经过指令微调和人类反馈的精细打磨，最终成为一个能够与人类流畅交互、完成各种任务的对话式AI系统。

RAG（Retrieval-Augmented Generation，检索增强生成） 是一种结合了信息检索技术与语言生成模型的人工智能技术。该技术通过从外部知识库中检索相关信息，并将其作为提示（Prompt）输入给大型语言模型（LLMs），以增强模型处理知识密集型任务的能力，如问答、文本摘要、内容生成等。
